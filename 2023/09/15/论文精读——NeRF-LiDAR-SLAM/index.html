<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">


<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">


<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"b2t":true,"scrollpercent":true,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>




  <meta name="description" content="本文是近期阅读NeRF + LiDAR SLAM 相关论文的精读笔记。  1、LiDAR-NeRF: Novel LiDAR View Synthesis via Neural Radiance Fields摘要翻译 我们引入了一种新的任务，即激光雷达传感器的新视图合成。虽然传统的基于模型的带有风格转移神经网络的激光雷达模拟器可以应用于渲染新视图，但由于渲染器依赖于显式的 3D 重建和利用游戏引">
<meta property="og:type" content="article">
<meta property="og:title" content="论文精读——NeRF+LiDAR SLAM">
<meta property="og:url" content="http://example.com/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/index.html">
<meta property="og:site_name" content="AppZ99&#39;s Blog">
<meta property="og:description" content="本文是近期阅读NeRF + LiDAR SLAM 相关论文的精读笔记。  1、LiDAR-NeRF: Novel LiDAR View Synthesis via Neural Radiance Fields摘要翻译 我们引入了一种新的任务，即激光雷达传感器的新视图合成。虽然传统的基于模型的带有风格转移神经网络的激光雷达模拟器可以应用于渲染新视图，但由于渲染器依赖于显式的 3D 重建和利用游戏引">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/image-20230915103955498.png">
<meta property="og:image" content="http://example.com/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/image-20230915104503985.png">
<meta property="og:image" content="http://example.com/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/image-20230915141712128.png">
<meta property="og:image" content="http://example.com/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/image-20230915151841407.png">
<meta property="og:image" content="http://example.com/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/image-20230915151852643.png">
<meta property="og:image" content="http://example.com/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/image-20230915152921994.png">
<meta property="og:image" content="http://example.com/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/image-20230915154914857.png">
<meta property="og:image" content="http://example.com/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/image-20230915160609405.png">
<meta property="og:image" content="http://example.com/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/image-20230915161101947.png">
<meta property="og:image" content="http://example.com/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/image-20230915162053107.png">
<meta property="og:image" content="http://example.com/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/image-20230915164446269.png">
<meta property="og:image" content="http://example.com/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/image-20230915170142448.png">
<meta property="og:image" content="http://example.com/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/image-20230919104523483.png">
<meta property="og:image" content="http://example.com/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/image-20230919134527285.png">
<meta property="og:image" content="http://example.com/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/image-20230922210431844.png">
<meta property="og:image" content="http://example.com/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/image-20230922221005120.png">
<meta property="og:image" content="http://example.com/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/image-20230922221358071.png">
<meta property="article:published_time" content="2023-09-15T02:23:45.000Z">
<meta property="article:modified_time" content="2023-10-07T13:35:24.714Z">
<meta property="article:author" content="AppZ99">
<meta property="article:tag" content="SLAM">
<meta property="article:tag" content="3D Reconstruction">
<meta property="article:tag" content="NeRF">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/image-20230915103955498.png">

<link rel="canonical" href="http://example.com/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>论文精读——NeRF+LiDAR SLAM | AppZ99's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">AppZ99's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">OHHHH</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="AppZ99">
      <meta itemprop="description" content="TKAW">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AppZ99's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          论文精读——NeRF+LiDAR SLAM
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-09-15 10:23:45" itemprop="dateCreated datePublished" datetime="2023-09-15T10:23:45+08:00">2023-09-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-10-07 21:35:24" itemprop="dateModified" datetime="2023-10-07T21:35:24+08:00">2023-10-07</time>
              </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>16k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>14 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <blockquote>
<p>本文是近期阅读NeRF + LiDAR SLAM 相关论文的精读笔记。</p>
</blockquote>
<h2 id="1、LiDAR-NeRF-Novel-LiDAR-View-Synthesis-via-Neural-Radiance-Fields"><a href="#1、LiDAR-NeRF-Novel-LiDAR-View-Synthesis-via-Neural-Radiance-Fields" class="headerlink" title="1、LiDAR-NeRF: Novel LiDAR View Synthesis via Neural Radiance Fields"></a>1、LiDAR-NeRF: Novel LiDAR View Synthesis via Neural Radiance Fields</h2><h3 id="摘要翻译"><a href="#摘要翻译" class="headerlink" title="摘要翻译"></a>摘要翻译</h3><blockquote>
<p>我们引入了一种新的任务，即激光雷达传感器的新视图合成。虽然传统的基于模型的带有风格转移神经网络的激光雷达模拟器可以应用于渲染新视图，但由于渲染器依赖于显式的 3D 重建和利用游戏引擎，其忽略了 LiDAR 点的重要属性，因此它们未能产生准确和逼真的 LiDAR 模式。据我们所知，我们提出了第一个可微的端到端激光雷达渲染框架 LiDAR-NeRF，利用神经辐射场（NeRF）来促进几何和 3D 点属性的联合学习，从而应对这一挑战。然而，简单地使用 NeRF 并不能取得令人满意的结果，因为它只关注学习单个像素而忽略局部信息，特别是在低纹理区域，导致几何性较差。为此，我们通过引入结构正则化方法来保留局部结构细节来解决这个问题。为了评估我们方法的有效性，我们建立了一个以物品为中心的多视图 LiDAR 数据集，称为 NeRF-MVL 。它包含从多个 LiDAR 传感器捕获的 360 度视点看到的 9 个类别物体的观察数据。我们在场景级数据集KITTI-360以及在我们的对象级 NeRF-MVL 上进行广泛的实验的结果表明，我们的 LiDAR-NeRF 显著优于基于模型的算法。</p>
</blockquote>
<span id="more"></span>
<h3 id="方法介绍"><a href="#方法介绍" class="headerlink" title="方法介绍"></a>方法介绍</h3><ul>
<li><strong>Overview</strong></li>
</ul>
<p><img src="/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/image-20230915103955498.png" alt="image-20230915103955498" style="zoom:80%;"></p>
<ul>
<li><strong>问题定义</strong></li>
</ul>
<p>如何通过输入一系列的 LiDAR 位姿以及相应的观测点，通过定义一个渲染函数的方式产生任意观测位姿下的新的观测图像？</p>
<ul>
<li><strong>Pipeline</strong></li>
</ul>
<p><img src="/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/image-20230915104503985.png" alt="image-20230915104503985" style="zoom:80%;"></p>
<ol>
<li><strong>LiDAR model and range image representation</strong></li>
</ol>
<p>这个步骤主要是通过 LiDAR 模型将 LiDAR 的测距信息以“伪图像”的形式进行表示。</p>
<p>这里使用了 <strong>球面投影模型（Spherical Projection）</strong> 将点云转换为 <strong>距离图（Range Images）</strong> ，具体公式如下：</p>
<script type="math/tex; mode=display">
\binom uv=\binom{\frac12[1-\arctan(y,x)/\pi]W}{\left[1-(\arcsin(z/r)+abs(FOV\_down))/FOV\right]H}</script><ol>
<li><strong>LiDAR-NeRF framework</strong></li>
</ol>
<p>将步骤一得到的距离图传入到 MLP 网络中分别输出 <strong>体密度（density）$\sigma$ ，强度（intensity）$i$ ，丢弃概率（ray-drop probability）$p$ </strong> ，接着对于三个输出值，均采用<strong>体渲染公式</strong>进行渲染。</p>
<script type="math/tex; mode=display">
\hat{C}(\mathbf{r})=\sum_{i=1}^NT_i(1-\exp(-\sigma_i\delta_i))\mathbf{c}_i,\text{where}T_i=\exp\left(-\sum_{j=1}^{i-1}\sigma_j\delta_j\right)</script><ol>
<li><strong>Structural characteristics</strong></li>
</ol>
<p>这一步骤主要是防止模型在 <strong>低纹理区域</strong> 的学习效果不佳所提出来的一种正则化的方法。主要做法是 <strong>在低纹理地区掩码取梯度</strong>。</p>
<script type="math/tex; mode=display">
\mathcal{L}_\mathrm{reg}=\left\|\hat{G_M}(\mathbf{R})-G_M(\mathbf{R})\right\|</script><p>其中$G_M()$ 为低纹理地区掩码取梯度的操作。</p>
<ol>
<li><strong>Loss function</strong></li>
</ol>
<p>最终构成损失函数：</p>
<script type="math/tex; mode=display">
\mathcal{L}_{total}=\mathcal{L}_{\text{distance}}+\lambda_{1}\mathcal{L}_{\text{intensity}}(\mathbf{r})+\lambda_{2}\mathcal{L}_{\text{raydrop}}(\mathbf{r})+\lambda_{3}\mathcal{L}_{\text{reg}}</script><p>其中距离损失取一范数，强度损失和丢弃损失均去二范数。</p>
<h2 id="2、SHINE-Mapping-Large-Scale-3D-Mapping-Using-Sparse-Hierarchical-Implicit-Neural-Representations"><a href="#2、SHINE-Mapping-Large-Scale-3D-Mapping-Using-Sparse-Hierarchical-Implicit-Neural-Representations" class="headerlink" title="2、SHINE-Mapping: Large-Scale 3D Mapping Using Sparse Hierarchical Implicit Neural Representations"></a>2、SHINE-Mapping: Large-Scale 3D Mapping Using Sparse Hierarchical Implicit Neural Representations</h2><h3 id="摘要翻译-1"><a href="#摘要翻译-1" class="headerlink" title="摘要翻译"></a>摘要翻译</h3><blockquote>
<p>大场景下的精密建图是大多数室外自动驾驶系统的一个重要组成部分。传统建图的方法所面临的挑战包括了平衡内存消耗和建图精度的难题。该文章主要解决了在大场景下利用3D LiDAR测量数据使用隐式表达进行重建的问题。我们使用一种具有稀疏性和可扩展性的基于八叉树，多层结构去学习和储存隐式特征。该隐式特征可以通过浅层的神经网络转化成符号距离值。我们利用二值交叉熵损失去优化我们局部特征，并使用3D测距信息作为监督。基于我们的隐式表达，我们设计了一种具有归一化的增量式建图系统去解决连续学习过程中的遗忘问题。我们的实验结果表明我们的3D重建对比目前的SOTA方法具有更准确，更完整和内存效率更高的效果。</p>
</blockquote>
<h3 id="方法介绍-1"><a href="#方法介绍-1" class="headerlink" title="方法介绍"></a>方法介绍</h3><ul>
<li><strong>问题定义</strong></li>
</ul>
<p>如何在大场景下利用 LiDAR 数据进行隐式表达进行重建？</p>
<ul>
<li><strong>Pipeline</strong></li>
</ul>
<p><img src="/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/image-20230915141712128.png" alt="image-20230915141712128" style="zoom:80%;"></p>
<ol>
<li><strong>Implicit Neural Map Representation</strong></li>
</ol>
<p>这一步的目的是 <strong>获得 LiDAR 数据的特征信息</strong> 。</p>
<p>首先将构建地图以 <strong>八叉树地图</strong> 进行划分，然后采用 <strong>不同尺度的网格</strong> 对 LiDAR 数据进行特征提取，最后将各层采样到的特征进行 <strong>累加操作</strong> ，作为网络的输入。</p>
<ol>
<li><strong>Training Pairs and Loss Function</strong></li>
</ol>
<p>训练过程中采用 <strong>采样点到射线端点的距离值直接作为监督值</strong> ，接着采用与 <strong>论文iSDF</strong> 中相同的策略，在传入 sigmoid 函数前先映射到 [0, 1]上，这样的做法是可以使得 <strong>距离表面更近的值获得更大的权重</strong> 。最终值经过 sigmoid函数后，采用 <strong>BCE(binary cross entropy)</strong> 作为损失函数。</p>
<script type="math/tex; mode=display">
L_\text{bce}=l_i\cdot\log(o_i)+(1-l_i)\cdot\log(1-o_i)</script><p>为了提高精度，添加 <strong>Eikonal loss</strong> 损失部分：</p>
<script type="math/tex; mode=display">
L_{\mathrm{batch}}=L_{\mathrm{bce}}+\lambda_{e}\underbrace{\left(\left\|\frac{\partial f_{\theta}\left(\boldsymbol{x}_{i}\right)}{\partial\boldsymbol{x}_{i}}\right\|-1\right)^{2}}_{\mathrm{Eikonal~loss}}</script><ol>
<li><strong>Incremental Mapping Without Forgetting</strong></li>
</ol>
<p>在基于学习的大规模重建问题中，随着场景的扩大，可能导致模型 <strong>更关注后续学习到的内容，而对前期学习到的内容产生了“遗忘”</strong> ，最终导致建图一致性较差的问题。</p>
<p>为此本文提出一项正则化方法以解决该系统的遗忘问题。添加了正则项损失如下：</p>
<script type="math/tex; mode=display">
L_\text{r}=\sum_{i\in A}\Omega_i(\theta_i^t-\theta_i^*)^2 \\
\Omega_i=\min\left(\Omega_i^*+\sum_{k=1}^N\left\Vert\frac{\partial L_{\text{bce}}(\boldsymbol{x}_k,l_k)}{\partial\theta_i}\right\Vert,\Omega_m\right)</script><p>其中 $\theta_i、\theta_i^*$ 分别表示当前的参数值和之前帧重已经收敛后的参数值，$\Omega_i$ 表示不同参数值的权重。</p>
<p>最终构成损失函数如下：</p>
<script type="math/tex; mode=display">
L_{\mathrm{incr}}=L_{\mathrm{bce}}+\lambda_{e}L_{\mathrm{eikonal}}+\lambda_{r}L_{r}</script><h2 id="3、NeRF-LOAM-Neural-Implicit-Representation-for-Large-Scale-Incremental-LiDAR-Odometry-and-Mapping"><a href="#3、NeRF-LOAM-Neural-Implicit-Representation-for-Large-Scale-Incremental-LiDAR-Odometry-and-Mapping" class="headerlink" title="3、NeRF-LOAM: Neural Implicit Representation for Large-Scale Incremental LiDAR Odometry and Mapping"></a>3、NeRF-LOAM: Neural Implicit Representation for Large-Scale Incremental LiDAR Odometry and Mapping</h2><h3 id="摘要翻译-2"><a href="#摘要翻译-2" class="headerlink" title="摘要翻译"></a>摘要翻译</h3><blockquote>
<p>使用LiDAR数据进行同时里程测量和建图是移动系统在大场景环境下完成完全自动驾驶的一项重要任务。然而，现存的绝大多数的基于LiDAR的方法优先考虑追踪质量多于重建质量。尽管最近开发的神经辐射场（NeRF）已经在室内环境下的隐式重建表现出富有前途的优势，但是，在大场景下使用增量式LiDAR数据进行同时里程测量和建图的问题仍然没能被探索。为了弥合这一缺陷，本文我们提出了一种新型的基于NeRF的LiDAR里程计与建图方法——NeRF-LOAM，其中包含了三种模块——神经里程计、神经建图以及mesh重建。所有这些模块使用了我们提出的神经符号距离函数，它将LiDAR点分离成在地面以及非地面两种类型的点以减少在Z轴上的漂移，同时优化里程计和体素嵌入，最后，生成关于环境的稠密光滑的mesh地图。而且，这种联合优化使得我们的NeRF-LOAM可以不用进行预训练以及展示了它应用在不同环境下的强大的泛化能力。在三个公开可用的数据集上的广泛评估显示我们的方法达到了SOTA表现，同时表现出大场景环境下使用LiDAR数据的强大的泛化能力。进一步，我们通过多重消融实验验证了我们所设计网络的有效性。</p>
</blockquote>
<h3 id="方法介绍-2"><a href="#方法介绍-2" class="headerlink" title="方法介绍"></a>方法介绍</h3><ul>
<li><strong>问题定义</strong></li>
</ul>
<p>基于 NeRF 的大场景下使用增量式LiDAR数据进行同时里程测量和建图的问题仍然没能被探索，本文提出了一种新型的基于 NeRF 的 LiDAR 里程计与建图方法——NeRF-LOAM。</p>
<ul>
<li><strong>Pipeline</strong></li>
</ul>
<p><img src="/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/image-20230915151841407.png" alt="image-20230915151841407"></p>
<p><img src="/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/image-20230915151852643.png" alt="image-20230915151852643"></p>
<ol>
<li><strong>Rays and points sampling</strong></li>
</ol>
<p>在当前给定的体素上，<strong>与该体素相交的射线并且和该体素产生交点</strong> ，设置一个阈值来避免表面遮挡的影响，由于 Lidar 光线是由扫描位姿变换得到的，所以<strong>每条射线包含姿态信息</strong>，这样优化时就能同时优化位姿和体素嵌入。</p>
<ol>
<li><strong>Neural SDF value</strong></li>
</ol>
<p>通过 <strong>三次线性插值</strong> 对采样点进行嵌入体素插值，然后将其输入到神经网络中：</p>
<script type="math/tex; mode=display">
\Psi(\boldsymbol{p}_s)=F_\theta(\operatorname{TriInpo}(\boldsymbol{p}_s,\boldsymbol{e}_1,...,\boldsymbol{e}_n))</script><ol>
<li><strong>Training SDF pairs</strong></li>
</ol>
<p><img src="/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/image-20230915152921994.png" alt="image-20230915152921994"></p>
<p>LiDAR 可以直接获得到物体表面的信息，即 SDF 值，但在 Z 轴上可能出现漂移，即上图中蓝色为真值， 红色为近似值，随着角度越来越小，其偏移量越来越大，故需要将地面分成 <strong>地面和非地面</strong> 两种情况：</p>
<script type="math/tex; mode=display">
\Phi(p_s)=\left\{\begin{array}{ll}(\mathbf{p_s-p})\mathbf{n_p}&\text{if}p\in\mathcal{G}\\\|\mathbf{p_s-p}\|&\text{else}\end{array}\right.</script><p>其中 $p_s、p、n_p$ 分别为采样点，激光沿着射线打到的点，激光沿着射线打到的点的法向量。</p>
<ol>
<li><p><strong>Optimization</strong></p>
<ol>
<li><p>自由空间损失：使得Lidar到截断正区域 $P_f$ 的各点 SDF 为截断距离 $Tr$。为了避免表面相交歧义，不考虑负截断区域，该损失函数在去除动态对象时非常重要。</p>
<script type="math/tex; mode=display">
\mathcal{L}_f=\dfrac{1}{|P_f|}\sum_{i=0}^{|P_f|}(\Psi(\boldsymbol{p}_i)-Tr)^2</script></li>
<li><p>SDF损失：</p>
<script type="math/tex; mode=display">
\mathcal{L}_s=\dfrac{1}{|P_s|}\sum_{i=0}^{|P_s|}(\Psi(\boldsymbol{p}_i)-\Phi(\boldsymbol{p}_i))^2</script></li>
<li><p>Eikonal loss：</p>
<script type="math/tex; mode=display">
\mathcal{L}_e=\frac{1}{|P_s|}\sum_{i=0}^{|P_s|}(\frac{\partial\Psi(\boldsymbol{p}_i)}{\partial\boldsymbol{p}_i}-1)^2</script></li>
</ol>
</li>
<li><p><strong>Dynamic voxel embeddings generation</strong></p>
</li>
</ol>
<p>通过 Neural Odometry 估计出的位姿可以将当前扫描的点转到世界坐标系中，任何不在现有体素中的点都被分配给新生成的体素，这些体素与对应体素嵌入一起被添加到八叉树中，为了快速定位所需的嵌入，将3D体素坐标编码为唯一的标量值，即莫顿代码。</p>
<p><img src="/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/image-20230915154914857.png" alt="image-20230915154914857" style="zoom:80%;"></p>
<ol>
<li>Mesh Reconstruction</li>
</ol>
<p>最终通过得到的 SDF 值，通过 <strong>Marching Cube</strong> 算法进行 mesh 重建。</p>
<h2 id="4、Efficient-Implicit-Neural-Reconstruction-Using-LiDAR"><a href="#4、Efficient-Implicit-Neural-Reconstruction-Using-LiDAR" class="headerlink" title="4、Efficient Implicit Neural Reconstruction Using LiDAR"></a>4、Efficient Implicit Neural Reconstruction Using LiDAR</h2><h3 id="摘要翻译-3"><a href="#摘要翻译-3" class="headerlink" title="摘要翻译"></a>摘要翻译</h3><blockquote>
<p>使用隐式神经表达创建模型场景几何已经表现其在精度、灵活性以及低存储使用的优势。之前的方法已经展示了使用彩色或者深度图所表现出来令人影响深刻的结果，但对于在低光照条件已经大规模场景的应用仍然是困难的。使用全局点云作为输入的方法需要精确的配准以及坐标标签的真值，其限制了它们的应用场景。本文中，我们提出了一种使用稀疏LiDAR点云和粗糙里程计去在几分钟内高效重建精细隐式占据地图的新方法。我们提出了一种直接监督三维空间，而不适用二维渲染的新的损失函数，该函数避免了信息的丢失。我们还设法以端到端的方式细化输入帧的姿势，在没有全局点云配准的情况下创建一致的几何图形。就我们所知，我们的方法是第一个仅使用LiDAR数据作为输入的隐式场景表达方法。在包括室内和室外场景的合成以及真实的数据集的实验上表明我们的方法是有效的，高效的和准确的，该方法获得了与现有使用稠密输入的方法相当的结果。</p>
</blockquote>
<h3 id="方法介绍-3"><a href="#方法介绍-3" class="headerlink" title="方法介绍"></a>方法介绍</h3><ul>
<li><strong>问题定义</strong></li>
</ul>
<p>本文中，我们提出了一种使用稀疏LiDAR点云和粗糙里程计去在几分钟内高效重建精细隐式占据地图的新方法。</p>
<ul>
<li><strong>Pipeline</strong></li>
</ul>
<p><img src="/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/image-20230915160609405.png" alt="image-20230915160609405"></p>
<ol>
<li><strong>Implicit Representation with Hash Encoder</strong></li>
</ol>
<p>采用多分辨率哈希编码，<strong>对不同分辨率下的特征值采用哈希编码</strong> ，然后将各个特征向量进行累加，传入神经网络中得到网格的 <strong>占据值</strong> 。该策略与 <strong>Instant-ngp</strong> 相同。</p>
<p><img src="/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/image-20230915161101947.png" alt="image-20230915161101947"></p>
<ol>
<li><strong>Direct Supervision</strong></li>
</ol>
<p>为了防止 <strong>观测歧义性（因为激光被遮挡后，物体厚度未知，在某一帧中可能是遮挡，在另一帧中可能是未遮挡）</strong>，对 <strong>BCE损失函数</strong> 进行改进， 这里主要的操作是 <strong>假设遮挡物体的厚度是符合正态分布的</strong>：</p>
<script type="math/tex; mode=display">
\mathcal{L}_d(\widetilde{\mathbf{p}}_i(z))=\left\{\begin{array}{ll}-\log(1-f_\theta(\widetilde{\mathbf{p}}_i(z))),&\text{for}~z<z_i\\-P_{occ}(z)\log(f_\theta(\widetilde{\mathbf{p}}_i(z))),&\text{for}~z\geq z_i\end{array}\right. \\
P_{occ}(z)=P(B>z-z_a)=1-F_B(z-z_a)</script><p><img src="/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/image-20230915162053107.png" alt="image-20230915162053107" style="zoom:80%;"></p>
<ol>
<li><strong>Joint Optimization</strong></li>
</ol>
<p>通过对 <strong>表面点附近进行采样</strong> ，结合 <strong>法向量损失（normal loss）</strong> 对重建平面进行光滑化。</p>
<script type="math/tex; mode=display">
\mathcal{L}_n(\mathbf{p}_i)=|1-\mathbf{n}(\mathbf{p}_i)\cdot\mathbf{n}(\mathbf{p}_i+\epsilon)| \\
\mathbf{n}(\mathbf{p}_i)=\frac{\nabla_{\mathbf{p}_i}f_\theta(\mathbf{p}_i)}{\left\|\nabla_{\mathbf{p}_i}f_\theta(\mathbf{p}_i)\right\|_2}</script><p>得到最终的损失函数如下：</p>
<script type="math/tex; mode=display">
\mathcal{L}=\frac1{N_fN_b}\left.\sum_{i=0}^{N_fN_b}\left(\frac1{m+k}\sum_{j=0}^{m+k}\lambda_d\mathcal{L}_d(\widetilde{\mathbf{p}}_i(z_j))+\lambda_n\mathcal{L}_n(\mathbf{p}_i)\right)\right.</script><h2 id="5、NF-Atlas-Multi-Volume-Neural-Feature-Fields-for-Large-Scale-LiDAR-Mapping"><a href="#5、NF-Atlas-Multi-Volume-Neural-Feature-Fields-for-Large-Scale-LiDAR-Mapping" class="headerlink" title="5、NF-Atlas: Multi-Volume Neural Feature Fields for Large Scale LiDAR Mapping"></a>5、NF-Atlas: Multi-Volume Neural Feature Fields for Large Scale LiDAR Mapping</h2><h3 id="摘要翻译-4"><a href="#摘要翻译-4" class="headerlink" title="摘要翻译"></a>摘要翻译</h3><blockquote>
<p>LiDAR建图是机器人领域中长期存在的一个问题。最近神经隐式表达领域的进步给机器人建图带来了全新的机会。在本文中，我们提出了多体素神经特征场，称为NF-Atlas，其可以将神经特征体素与位姿图优化进行联系。通过将神经特征体素视为位姿图节点，将体素之间的相对位姿视为位姿图边，整个神经特征场都变为局部刚性和全局弹性。对于局部，神经特征体素部署了一个稀疏特征八叉树和小型的MLP网络对局部子图的SDF进行编码，且可以进行语义选取。使用这种结构去学习地图使得可以解决基于概率地图的端到端的最大后验问题。全局部分，地图由一个个单独的体素构成，避免了增量建图中灾难性的遗忘问题。更进一步的，当回环产生的时候，伴随着基于表示的弹性位置图只更新器神经体素的原点信息而不用进行重映射。最终，NF-Atlas的这些功能都被验证有效。由于稀疏性和基于优化的公式，NF-Atlas在模拟和真实数据集上的准确性、效率和内存使用方面表现出具有竞争力的性能。</p>
</blockquote>
<h3 id="方法介绍-4"><a href="#方法介绍-4" class="headerlink" title="方法介绍"></a>方法介绍</h3><ul>
<li><strong>问题定义</strong></li>
</ul>
<p>在本文中，我们提出了多体素神经特征场，称为NF-Atlas，其可以将神经特征体素与位姿图优化进行联系。</p>
<ol>
<li><strong>Neural Feature Volume</strong></li>
</ol>
<p>采用 <strong>八叉树</strong> 存储局部特征值，然后将其特征值进行 <strong>三次线性插值</strong> 后累加，输入到神经网络中。</p>
<script type="math/tex; mode=display">
\Psi(\mathbf{p})=\sum_{i=0}^{K-1}triInterp(\psi_{i,j\in\mathcal{N}(\mathbf{p})})</script><p>将位置信息进行 <strong>编码操作</strong>，这里的编码与 NeRF 中的相同。将特征向量与位置编码向量同时输入到神经网络中：</p>
<script type="math/tex; mode=display">
s=f_\theta(\Psi(\mathbf{p}),\gamma(\mathbf{p})) \\
\gamma(\mathbf{p})=\begin{bmatrix}\cos(2^0\pi\mathbf{p}),\sin(2^0\pi\mathbf{p}),\ldots&\cos(2^{L-1}\pi\mathbf{p}),\sin(2^{L-1}\pi\mathbf{p})\end{bmatrix}</script><p><img src="/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/image-20230915164446269.png" alt="image-20230915164446269" style="zoom:80%;"></p>
<ol>
<li><strong>Differentiable Range Approximation</strong> </li>
</ol>
<p>采用 <strong>NeuS</strong> 中的渲染方式，其渲染公式与体渲染公式组成相同：</p>
<script type="math/tex; mode=display">
r=\sum_{n=1}^NT_n\alpha_n\rho_n \\
T_{n}=\prod_{m=1}^{n-1}(1-\alpha_{m}) \\
\alpha_n=\max\left(\frac{\Phi(s_n)-\Phi(s_{n+1})}{\Phi(s_n)},0\right) \\
\begin{aligned}\Phi(x)=(1+e^{-\xi x})^{-1}\end{aligned}</script><p>采样方式参考 <em>Neural 3D Reconstruction in the Wild</em> .</p>
<ol>
<li><p><strong>Likelihood Measurement Model</strong> </p>
<ol>
<li><p>Range Measurements</p>
<p>采用最经典的占据概率地图建模：</p>
<script type="math/tex; mode=display">
p_r(r|\mathbf{o},\mathbf{d},\theta,\Psi)=\frac{1}{\sigma_r\sqrt{2\pi}}\exp\left\{-\frac{(\hat{r}-r(\mathbf{o},\mathbf{d}))^2}{2\sigma_r^2}\right\}</script><p>对不同距离的构建不同的概率模型：</p>
<script type="math/tex; mode=display">
p_s(s|\mathbf{p},\theta,\Psi)=\begin{cases}p_{near}(s|\mathbf{p},\theta,\Psi)&|b|\leq\tau\\p_{far}(s|\mathbf{p},\theta,\Psi)&o.w.\end{cases} \\
p_{near}(s|\mathbf{p},\theta,\Psi)=\frac{1}{2\lambda}\exp\left\{-\frac{|b-s(\mathbf{p})|}{\lambda}\right\} \\
p_{far}(s|\mathbf{p},\theta,\Psi)=\eta\exp\left\{-\max(0,e^{-\beta s(\mathbf{p})}-1,s(\mathbf{p})-b)\right\}</script></li>
<li><p>Semantic Measurements</p>
<p>这部分是对语义进行测量，不太明白。</p>
</li>
</ol>
</li>
<li><p><strong>Probabilistic Mapping</strong></p>
</li>
</ol>
<p>最终得到 <strong>最大后验概率问题</strong> ：</p>
<script type="math/tex; mode=display">
\tilde{\theta},\tilde{\Psi},\tilde{\mathbf{o}},\tilde{\mathbf{d}}=\arg\max\prod\underbrace{p_r\cdot p_s\cdot p_c}_{\text{Likelihood}}\cdot\underbrace{p_e\cdot p_h}_{\text{Prior}}
\\
p_e(\theta,\Psi)=\frac{1}{\sigma_e\sqrt{2\pi}}\exp\left\{-\frac{(1-\|\nabla_\mathbf{p}s(\mathbf{p})\|)^2}{2\sigma_e^2}\right\}
\\
p_h(\theta,\Psi)=\frac{1}{\sigma_h\sqrt{2\pi}}\exp\left\{-\frac{\|\nabla_\mathbf{p}s(\mathbf{p})-\nabla_\mathbf{p}s(\mathbf{p}+\Delta\mathbf{p})\|^2}{2\sigma_h^2}\right\}</script><ol>
<li><strong>Loop Closure without Remapping</strong></li>
</ol>
<p>本方法在进行回环后，<strong>仅更新每个子图的原点位置</strong>，然后将子图进行拼接，避免了轨迹更新后的地图重映射。</p>
<p><img src="/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/image-20230915170142448.png" alt="image-20230915170142448"></p>
<ol>
<li><strong>On-demand Global Mapping</strong></li>
</ol>
<p>可以按需从世界坐标系中得到各个体素的 SDF 值：</p>
<script type="math/tex; mode=display">
s=f_{\theta_l}(\Psi_l(\mathbf{T}_l^{-1}\mathbf{p}^W),\gamma(\mathbf{T}_l^{-1}\mathbf{p}^W))</script><ol>
<li><strong>Loop Closure Updating</strong> </li>
</ol>
<p>通过比较两个子图的最大似然值进行回环更新：</p>
<script type="math/tex; mode=display">
\tilde{\mathbf{o}}_{j},\tilde{\mathbf{d}}_{j}=\arg\max p_{r}(r_{j}|\mathbf{o}_{j},\mathbf{d}_{j},\theta_{l},\Psi_{l})</script><h2 id="6、LONER-LiDAR-Only-Neural-Representations-for-Real-Time-SLAM"><a href="#6、LONER-LiDAR-Only-Neural-Representations-for-Real-Time-SLAM" class="headerlink" title="6、LONER: LiDAR Only Neural Representations for Real-Time SLAM"></a>6、LONER: LiDAR Only Neural Representations for Real-Time SLAM</h2><h3 id="摘要翻译-5"><a href="#摘要翻译-5" class="headerlink" title="摘要翻译"></a>摘要翻译</h3><blockquote>
<p>本文提出了 LONER，第一个使用神经隐式场景表达的实时 LiDAR SLAM算法。现存的在大场景下的 LiDAR 隐式建图方法展现了富有前途的结果，但他们要么需要位姿真值，要么运行速度远达不到实时性。相反，LONER使用了 LiDAR 数据去实时训练一个 MLP 去预测稠密地图，同时同步估计传感器的轨迹信息。为了达到实时性的表现，本文提出了一种新的信息论损失函数，该函数解释了整个在线训练的过程中，不同地区的地图可以被不同程度地学习的事实。本方法在两个公开数据集上定性定量地进行了评估。评估结果说明了所提出的损失函数比其他使用深度监督的神经隐式框架的损失函数收敛速度更快，几何重建效果更加精确。最终，本文展示了 LONER 跟 SOTA 的 LiDAR SLAM系统相比具有竞争性的轨迹估计能力，同时也产生了与现存的需要位姿真值的隐式建图方法相比具有竞争性的稠密地图。</p>
</blockquote>
<h3 id="方法介绍-5"><a href="#方法介绍-5" class="headerlink" title="方法介绍"></a>方法介绍</h3><ul>
<li><strong>问题定义</strong></li>
</ul>
<p>设计一个实时的基于神经隐式表达的 LiDAR SLAM 算法。</p>
<ul>
<li><strong>Pipeline</strong> </li>
</ul>
<p><img src="/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/image-20230919104523483.png" alt="image-20230919104523483" style="zoom:80%;"></p>
<ol>
<li><strong>Tracking</strong></li>
</ol>
<p>Tracking部分主要采用 ICP 算法进行跟踪，运动过程采用匀速运动模型。</p>
<ol>
<li><strong>Implicit Map Representation</strong></li>
</ol>
<p>隐式表达采用 Instant-npg 模型，渲染还是采用体渲染公式：</p>
<script type="math/tex; mode=display">
T_i=\exp(-\sum_{j=1}^{i-1}\sigma_j\delta_j) \\
w_i=T_i\sigma_i \\
\hat{D}(\vec{r})=\sum_{i=1}^Nw_it_i.</script><ol>
<li><strong>Mapping</strong><ol>
<li>关键帧：当添加关键帧时，将当前关键帧以及之前的 $N_W-1$ 个关键帧进行优化更新；</li>
<li>优化：通过选取的 $N_W$ 个关键帧进行位姿优化；</li>
<li>采样：随机按选取 $N_R$ 条射线，再通过占据网格从每条射线采样 $N_S$ 个深度采样点。</li>
</ol>
</li>
<li><strong>JS Dynamic Margin Loss Function</strong></li>
</ol>
<script type="math/tex; mode=display">
\mathcal{L}(\Theta)=\mathcal{L}_{JS}+\lambda_1\mathcal{L}_{depth}+\lambda_2\mathcal{L}_{sky}.</script><ul>
<li><p>JS Loss</p>
<p>由 LOS Loss修改得到，主要根据目标分布和预测样本分布之间的相似性为每一条射线设置一个动态边界 $\epsilon$ 。学习过的区域设置较低的边距来细化学习到的几何形状，未学习的部分设置较宽的区域加快收敛。</p>
<p><img src="/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/image-20230919134527285.png" alt="image-20230919134527285" style="zoom:80%;"></p>
<script type="math/tex; mode=display">
\mathcal{L}_{JS}(\Theta)=\underbrace{\|w_i^*-w_i\|_1}_{\text{Primary Loss}}+\underbrace{\|1-\sum_iw_i\|_1}_{\text{Opacity Loss}} \\
w_{i}^{*}=\mathcal{K}_{\epsilon}(t_{i}-z^{*}) \\
\mathcal{K}_\epsilon=\mathcal{N}(0,(\epsilon/3)^2) \\
\epsilon_{dyn}=\epsilon_{min}(1+\alpha\mathbf{J}^*) \\
\mathbf{J}^*=\begin{cases}0&JS(G||S)<JS_{min}\\JS_{max}&JS(G||S)>JS_{max}\\JS(G||S)&\text{otherwise},\end{cases}</script></li>
<li><p>Depth Loss</p>
<p>深度损失为测量值和渲染深度之间的误差的平方：</p>
<script type="math/tex; mode=display">
\mathcal{L}_{depth}(\Theta)=\|\hat{D}(\vec{r})-z^*\|_2^2</script></li>
<li><p>Sky Loss</p>
<p>这个损失主要是区分天空部分的。在没有接受到激光雷达反射信号的情况下，若仰角满足一定的阈值，则判定为天空部分。</p>
<script type="math/tex; mode=display">
\mathcal{L}_{sky}(\Theta)=\|w\|_1.</script></li>
</ul>
<ol>
<li><strong>Meshing</strong></li>
</ol>
<p>采用 Marching cube 算法进行重建。</p>
<h2 id="7、CLONeR-Camera-Lidar-Fusion-for-Occupancy-Grid-aided-Neural-Representations"><a href="#7、CLONeR-Camera-Lidar-Fusion-for-Occupancy-Grid-aided-Neural-Representations" class="headerlink" title="7、CLONeR: Camera-Lidar Fusion for Occupancy Grid-aided Neural Representations"></a>7、CLONeR: Camera-Lidar Fusion for Occupancy Grid-aided Neural Representations</h2><h3 id="摘要翻译-6"><a href="#摘要翻译-6" class="headerlink" title="摘要翻译"></a>摘要翻译</h3><blockquote>
<p>NeRF领域的最新进展实现了SOTA级别的新视角合成并促进了场景属性的稠密估计。然而NeRF通常无法再室外、无边界的场景中使用，这些场景实在非常稀疏的视图下拍摄的，同时这些场景内容主要集中在远离相机的位置，这是野外机器人应用的典型情况。特别地，NeRF风格的算法在如下方面表现不佳：（1）当位姿多样性下的视图比较少的时候；（2）当场景包含饱和度和阴影时；（3）当使用精细结构对无边界大场景进行精细采样的时候导致的计算密集时。本文提出了CLONeR，它显著改进了NeRF，允许它对从稀疏输入的传感器视图观察到的大型无界户外驾驶场景建模。这是通过将NeRF框架内的占用和颜色学习解耦为分别使用激光雷达和相机数据训练的单独的多层感知器（MLP）来实现的。此外，本文还提出了一种新的方法，在NeRF模型的基础上构建可微分的3D占用网格图（OGM），并利用该占用网格改进沿射线的点采样，以在度量空间中进行体积绘制。通过对KITTI数据集场景的大量定量和定性实验，本文证明了当在稀疏输入数据上训练时，所提出的方法在新视图合成和密集深度预测任务上都优于SOTA的NeRF模型。</p>
</blockquote>
<h3 id="方法介绍-6"><a href="#方法介绍-6" class="headerlink" title="方法介绍"></a>方法介绍</h3><ul>
<li><strong>问题定义</strong></li>
</ul>
<p>设计一种方法，允许它对从稀疏输入的传感器视图观察到的大型无界户外驾驶场景建模。</p>
<ul>
<li><strong>Pipeline</strong></li>
</ul>
<p><img src="/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/image-20230922210431844.png" alt="image-20230922210431844"></p>
<ol>
<li><strong>CLONeR World Coordinate System</strong></li>
</ol>
<p>这一步与 NeRF 相同，通过外部源如GNSS系统将相机和 LiDAR 的 Z 轴指向相机本身，然后将 LiDAR 数据置于感兴趣的区域内。</p>
<p>感兴趣区域的定义：通过相机本质参数和用户定义，确定视锥点的八个位置，然后通过计算一个放缩因子，将所有的位姿置于该放缩区域内。</p>
<ol>
<li><strong>Integrating LiDAR Measurements into NeRF</strong></li>
</ol>
<p>这一步主要是通过激光雷达模型将测量点置于整个世界坐标系下。</p>
<ol>
<li><strong>Decoupled NeRF Model</strong></li>
</ol>
<p>这部分主要是本论文的核心，借用 Instant-ngp 的框架，拆分成 Sigma MLP 和 RGB MLP 两个网络。LiDAR 数据传入 Sigma MLP 中， 相机数据传入 Sigma MLP 和 RGB MLP 中。其中位置信息编码采用原 NeRF 中的位置编码，观测角度编码采用 Spherical Harmonics 论文中提到的一种编码方式。</p>
<p>在本文的模型中采用了全针孔模型相机，在多层采样过程中对图像信息进行二次线性插值以得到更高分辨率的像素RGB真值。</p>
<ol>
<li><strong>Loss Functions for NeRF MLPs</strong></li>
</ol>
<p>主要参考了 URF 中的损失函数。</p>
<script type="math/tex; mode=display">
\mathcal{L}(\Theta,\Phi)=\lambda_{1}\mathcal{L}_{\text{sight}}(\Theta)+\mathcal{L}_{\text{opacity}_{1}}(\Theta)+\lambda_{2}\mathcal{L}_{\text{color}}(\Phi) \\
\mathcal{L}_{\text{sight}}(\Theta)=\|w_i-w_i^*\|_1\\
\mathcal{L}_{\text{opacity}_1}(\Theta)=\|1-\sum_iw_i\| \\
\mathcal{L}_{\text{color}}(\Phi)=\|\hat{\vec{C}}(\vec{r})-\vec{C}(\vec{r})\|_1</script><ol>
<li><strong>Occupancy Grid Mapping</strong></li>
</ol>
<p><img src="/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/image-20230922221005120.png" alt="image-20230922221005120"></p>
<p>把OGM作为一个优化问题，使用随机梯度下降法去优化，然后使用这个学习到的OGM去代替原NeRF中的粗糙MLP.</p>
<p>通过占据网格地图更新公式，改进成优化函数：</p>
<script type="math/tex; mode=display">
l_{n,k}=l_{n-1,k}+\mathcal{S}(k,z_n)-l_0 \\
\gamma_k:=\gamma_k-\alpha\nabla\mathcal{L}(\mathcal{O},z_n) \\
-\alpha\nabla\mathcal{L}(\mathcal{O},z_n)=\mathcal{S}(k,z_n)-\gamma_0</script><p>采样一共分为两步：1、在射线上采样 N/2 个点；2、对占据区域进行剩下的 N/2 个点的采样：</p>
<p>LiDAR 射线上的占据概率：</p>
<script type="math/tex; mode=display">
p_{\vec\zeta_j}=\frac{1}{1+\exp(-l_{\vec\zeta_j})}</script><p>定义随机梯度函数：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial\mathcal{L}}{\partial l_{\vec{\beta}_{j}}}=g(\beta_{j},z_{n})& =l_{\mathbf{free}}\mathcal{U}((z_{t}-\delta)-\beta_{j})+  \\
&-l_\text{осс}{ \mathcal{U}}(\beta_j-(z_t-\delta)){\mathcal{U}}((z_t+\delta)-\beta_j)
\end{aligned}</script><ol>
<li><strong>Training Details</strong></li>
</ol>
<p>训练的三个阶段：1、只输入LiDAR训练S网络；2、冻结1中的权重，只输入相机数据训练C网络；3、两种数据共同训练S、C网络。</p>
<h2 id="8、Urban-Radiance-Fields"><a href="#8、Urban-Radiance-Fields" class="headerlink" title="8、Urban Radiance Fields"></a>8、Urban Radiance Fields</h2><h3 id="摘要翻译-7"><a href="#摘要翻译-7" class="headerlink" title="摘要翻译"></a>摘要翻译</h3><blockquote>
<p>本工作的目标是从扫描平台获取到的数据执行三维重建以及新视图合成，这些平台普遍被部署在城市室外环境下的世界映射中。给定一系列从相机和扫描仪通过通过室外场景移动得到的RGB图像和雷达扫描数据，我们生成了一个可以从中提取三维表面并合成新的RGB图像的模型。我们的方法扩展了神经辐射场，它已经被证明可以在可控设定下对小场景生成逼真的新图片，利用异步捕获的激光雷达数据解决捕获图片之间的曝光变化，同时利用预测的图像分割去监测指向天空的光线的体密度。这三个扩展中的每一个在街景数据集上的实验中提供了显著的性能提升。我们的系统产生了SOTA的三维表面重建，并与传统方法和最近的神经表示方法相比我们得到了更高质量的新视角合成图像。</p>
</blockquote>
<h3 id="方法介绍-7"><a href="#方法介绍-7" class="headerlink" title="方法介绍"></a>方法介绍</h3><ul>
<li><strong>问题定义</strong></li>
</ul>
<p>如何在给定一系列从相机和扫描仪通过通过室外场景移动得到的RGB图像和雷达扫描数据，生成了一个可以从中提取三维表面并合成新的RGB图像的模型。</p>
<ul>
<li><strong>Overview</strong></li>
</ul>
<p><img src="/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/image-20230922221358071.png" alt="image-20230922221358071" style="zoom:80%;"></p>
<ol>
<li><strong>Loss Function</strong></li>
</ol>
<p>总的损失函数：</p>
<script type="math/tex; mode=display">
\underset{\boldsymbol{\theta},\{\boldsymbol{\beta}_i\}}{\operatorname*{\arg\min}}\underbrace{\mathcal{L}_{\mathrm{rgb}}(\boldsymbol{\theta},\{\beta_i\})+\mathcal{L}_{\mathrm{seg}}(\boldsymbol{\theta})}_{Sec.4.1}+\underset{Sec.4.2}{\operatorname*{\underbrace{\mathcal{L}_{\mathrm{depth}}(\boldsymbol{\theta})+\mathcal{L}_{\mathrm{sight}}(\boldsymbol{\theta})}}}</script><ol>
<li><strong>Photometric-based Losses</strong></li>
</ol>
<p>RGB损失和 NeRF 中相同：</p>
<script type="math/tex; mode=display">
\mathcal{L}_\mathrm{rgb}(\boldsymbol{\theta},\{\beta_i\})=\sum\mathbb{E}_{\mathbf{r}\sim I_i}\left[||\boldsymbol{C}(\mathbf{r};\beta_i)-\boldsymbol{C}_i^\mathrm{gt}(\mathbf{r})||_2^2\right]</script><p>但其中的 RGB 函数有所不同：</p>
<script type="math/tex; mode=display">
\boldsymbol{C}(\mathbf{r};\beta_i)=\int_{t_n}^{t_f}w(t)\cdot\underbrace{\Gamma(\beta_i)}_{\text{Sec. B.2}}\cdot\mathbf{c}(t)dt+\underbrace{\mathbf{c}_{\text{sky}}(\mathbf{d})}_{\text{Sec. 4.1.2}}</script><ol>
<li><strong>Exposure compensation</strong></li>
</ol>
<p>对每一张图片进行曝光的潜在编码是过度参数化的，这会影响到非曝光的相关误差。本篇文章的解决方法是利用一个网络生成一个3*3的矩阵，用来对图片的颜色做仿射变换，这个映射将白平衡和曝光参数以更具有约束性的函数去建模。</p>
<script type="math/tex; mode=display">
\Gamma(\beta_i)=\Gamma(\beta_i;\boldsymbol{\theta}):\mathbb{R}^B\to\mathbb{R}^{3\times3}</script><ol>
<li><strong>Sky modeling</strong></li>
</ol>
<p>本篇文章使用了spherical radiance map。首先用一个预训练模型语义分割出天空的部分，然后加入一个 Loss，使得其对于天空部分的体密度预测为 0：</p>
<script type="math/tex; mode=display">
\mathcal{L}_{seg}(\theta)=\mathbb{E}_{r\sim I_i}[\mathcal{S}_i(r)\int_{t_n}^{t_f}w(t)^2dt]</script><ol>
<li><strong>Lidar losses</strong></li>
</ol>
<p>首先是深度损失：</p>
<script type="math/tex; mode=display">
\mathcal{L}_{\mathrm{depth}}(\boldsymbol{\theta})=\mathbb{E}_{\mathbf{r}\sim\mathcal{D}}\left[(\hat{z}-z)^{2}\right]\hat{z}=\int_{t_{n}}^{t_{f}}w(t)\cdot tdt</script><p>接着是 Line-of-sight priors 的设计：</p>
<p>光路上的介质不影响其最终的颜色，我们期望辐射亮度集中在沿射线的一个点上:</p>
<script type="math/tex; mode=display">
C(\mathbf{r}_{\ell})\equiv\mathbf{c}(\mathbf{r}_{\ell})\quad\text{iff}\quad w(t)=\delta(t)\\
\mathcal{L}_{\mathrm{sight}}(\boldsymbol{\theta})=\mathbb{E}_{\mathbf{r}\sim\mathcal{D}}\left[\int_{t_n}^{t_f}\left(w(t)-\delta(z)\right)^2dt\right]</script><p>将上式分为三个部分：</p>
<script type="math/tex; mode=display">
\mathcal{L}_{\mathrm{sight}}(\boldsymbol{\theta})=\underbrace{\mathcal{L}_{\mathrm{empty}}(\boldsymbol{\theta})}_{t\in[t_n,z-\epsilon]}+\underbrace{\mathcal{L}_{\mathrm{near}}(\boldsymbol{\theta})}_{t\in[z-\epsilon,z+\epsilon]}+\underbrace{\mathcal{L}_{\mathrm{dist}}(\boldsymbol{\theta})}_{t\in[z+\epsilon,t_f]}</script><p>目的就是为了在观测到的位置具有更大的权重值。</p>
<p>为了数值上便于处理，采样积分为1的核函数代替狄拉克函数，该核函数满足正态分布：</p>
<script type="math/tex; mode=display">
\mathcal{K}_{\epsilon}(x)=\mathcal{N}(0,(\epsilon/3)^{2})</script><p>给出各个部分的定义：</p>
<script type="math/tex; mode=display">
\mathcal{L}_{\text{near}}(\boldsymbol{\theta})=\mathbb{E}_{\mathbf{r}\sim\mathcal{D}}\left[\int_{z-\epsilon}^{z+\epsilon}\left(w(t)-\mathcal{K}_\epsilon(t-z)\right)^2dt\right] \\
\mathcal{L}_{\mathrm{empty}}(\boldsymbol{\theta})=\mathbb{E}_{\mathbf{r}\sim\mathcal{D}}\left[\int_{t_n}^{z-\epsilon}w(t)^2dt\right] \\
\mathcal{L}_{\mathrm{dist}}(\boldsymbol{\theta})=\mathbb{E}_{\mathbf{r}\sim\mathcal{D}}\left[\int_{z+\epsilon}^{t_f}w(t)^2dt\right]</script><p>其中最后一项主要是为了保证射线上整个权重值 $w(t)$ 为1。</p>

    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>AppZ99
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://example.com/2023/09/15/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-LiDAR-SLAM/" title="论文精读——NeRF+LiDAR SLAM">http://example.com/2023/09/15/论文精读——NeRF-LiDAR-SLAM/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/SLAM/" rel="tag"><i class="fa fa-tag"></i> SLAM</a>
              <a href="/tags/3D-Reconstruction/" rel="tag"><i class="fa fa-tag"></i> 3D Reconstruction</a>
              <a href="/tags/NeRF/" rel="tag"><i class="fa fa-tag"></i> NeRF</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/09/07/MVG-%E9%9A%8F%E8%AE%B0/" rel="prev" title="MVG-随记">
      <i class="fa fa-chevron-left"></i> MVG-随记
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/10/06/%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB%E2%80%94%E2%80%94NeRF-SLAM/" rel="next" title="论文精读——NeRF-SLAM">
      论文精读——NeRF-SLAM <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>


      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1%E3%80%81LiDAR-NeRF-Novel-LiDAR-View-Synthesis-via-Neural-Radiance-Fields"><span class="nav-text">1、LiDAR-NeRF: Novel LiDAR View Synthesis via Neural Radiance Fields</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%91%98%E8%A6%81%E7%BF%BB%E8%AF%91"><span class="nav-text">摘要翻译</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%95%E4%BB%8B%E7%BB%8D"><span class="nav-text">方法介绍</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2%E3%80%81SHINE-Mapping-Large-Scale-3D-Mapping-Using-Sparse-Hierarchical-Implicit-Neural-Representations"><span class="nav-text">2、SHINE-Mapping: Large-Scale 3D Mapping Using Sparse Hierarchical Implicit Neural Representations</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%91%98%E8%A6%81%E7%BF%BB%E8%AF%91-1"><span class="nav-text">摘要翻译</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%95%E4%BB%8B%E7%BB%8D-1"><span class="nav-text">方法介绍</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3%E3%80%81NeRF-LOAM-Neural-Implicit-Representation-for-Large-Scale-Incremental-LiDAR-Odometry-and-Mapping"><span class="nav-text">3、NeRF-LOAM: Neural Implicit Representation for Large-Scale Incremental LiDAR Odometry and Mapping</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%91%98%E8%A6%81%E7%BF%BB%E8%AF%91-2"><span class="nav-text">摘要翻译</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%95%E4%BB%8B%E7%BB%8D-2"><span class="nav-text">方法介绍</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4%E3%80%81Efficient-Implicit-Neural-Reconstruction-Using-LiDAR"><span class="nav-text">4、Efficient Implicit Neural Reconstruction Using LiDAR</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%91%98%E8%A6%81%E7%BF%BB%E8%AF%91-3"><span class="nav-text">摘要翻译</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%95%E4%BB%8B%E7%BB%8D-3"><span class="nav-text">方法介绍</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5%E3%80%81NF-Atlas-Multi-Volume-Neural-Feature-Fields-for-Large-Scale-LiDAR-Mapping"><span class="nav-text">5、NF-Atlas: Multi-Volume Neural Feature Fields for Large Scale LiDAR Mapping</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%91%98%E8%A6%81%E7%BF%BB%E8%AF%91-4"><span class="nav-text">摘要翻译</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%95%E4%BB%8B%E7%BB%8D-4"><span class="nav-text">方法介绍</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6%E3%80%81LONER-LiDAR-Only-Neural-Representations-for-Real-Time-SLAM"><span class="nav-text">6、LONER: LiDAR Only Neural Representations for Real-Time SLAM</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%91%98%E8%A6%81%E7%BF%BB%E8%AF%91-5"><span class="nav-text">摘要翻译</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%95%E4%BB%8B%E7%BB%8D-5"><span class="nav-text">方法介绍</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7%E3%80%81CLONeR-Camera-Lidar-Fusion-for-Occupancy-Grid-aided-Neural-Representations"><span class="nav-text">7、CLONeR: Camera-Lidar Fusion for Occupancy Grid-aided Neural Representations</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%91%98%E8%A6%81%E7%BF%BB%E8%AF%91-6"><span class="nav-text">摘要翻译</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%95%E4%BB%8B%E7%BB%8D-6"><span class="nav-text">方法介绍</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#8%E3%80%81Urban-Radiance-Fields"><span class="nav-text">8、Urban Radiance Fields</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%91%98%E8%A6%81%E7%BF%BB%E8%AF%91-7"><span class="nav-text">摘要翻译</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B9%E6%B3%95%E4%BB%8B%E7%BB%8D-7"><span class="nav-text">方法介绍</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="AppZ99"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">AppZ99</p>
  <div class="site-description" itemprop="description">TKAW</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">30</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">35</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/APPZ99" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;APPZ99" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:925455893@qq.com" title="E-Mail → mailto:925455893@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh-CN" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

      <div id="music163player">
        <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=1872048272&auto=1&height=66">
        </iframe>
      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">AppZ99</span>

<script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">109k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">1:40</span>
</div>

        








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,0' opacity='0.5' zIndex='-1' count='150' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
//    window.MathJax = {
//      loader: {
//
//        source: {
//          '[tex]/amsCd': '[tex]/amscd',
//          '[tex]/AMScd': '[tex]/amscd'
//        }
//      },
//      tex: {
//        inlineMath: {'[+]': [['$', '$']]},
//
//        tags: 'ams'
//      },
//      options: {
//        renderActions: {
//          findScript: [10, doc => {
//            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
//              const display = !!node.type.match(/; *mode=display/);
//              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
//              const text = document.createTextNode('');
//              node.parentNode.replaceChild(text, node);
//              math.start = {node: text, delim: '', n: 0};
//              math.end = {node: text, delim: '', n: 0};
//              doc.math.push(math);
//            });
//          }, '', false],
//          insertedScript: [200, () => {
//            document.querySelectorAll('mjx-container').forEach(node => {
//              let target = node.parentNode;
//              if (target.nodeName.toLowerCase() === 'li') {
//                target.parentNode.classList.add('has-jax');
//              }
//            });
//          }, '', false]
//        }
//      }
//    };
    window.MathJax = {
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax@2.7.8/unpacked/MathJax.js?config=TeX-MML-AM_CHTML';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/hijiki.model.json"},"display":{"position":"right","width":150,"height":500},"mobile":{"show":true}});</script></body>
</html>
